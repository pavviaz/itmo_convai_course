version: "2.3"

services:  
  worker:
    restart: "no"
    mem_limit: 16G
    container_name: worker

    build: 
      context: ./llava_worker
      dockerfile: Dockerfile
    command: bash -c "uvicorn main:app --host 0.0.0.0 --port ${WORKER_PORT}"

    environment:
      - NVIDIA_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES}
      - TRANSFORMERS_CACHE=${TRANSFORMERS_CACHE}

    volumes:
      - ./llava_worker:/llava_worker

      # TEST ONLY
      - /home/shace_linux/.cache/huggingface/hub:/cache/

    runtime: nvidia

    ports:
      - ${WORKER_PORT}:${WORKER_PORT}
      
  neural_api:
    restart: "no"
    mem_limit: 2G
    container_name: neural_api

    build: 
      context: ./neural_api
      dockerfile: Dockerfile
    command: bash -c "uvicorn app.main:app --host 0.0.0.0 --port ${NEURAL_API_PORT}"

    volumes:
      - ./neural_api:/neural_api

    environment:
      - WORKER_PORT=${WORKER_PORT}

    ports:
      - ${NEURAL_API_PORT}:${NEURAL_API_PORT}

  # dff_api:
  #   restart: "no"
  #   mem_limit: 2G
  #   container_name: dff_api

  #   build: 
  #     context: ./dff_api
  #     dockerfile: Dockerfile
  #   command: bash -c "python3 app/dff_main.py"

  #   volumes:
  #     - ./dff_api:/dff_api

  #   environment:
  #     - TG_BOT_TOKEN=${TG_BOT_TOKEN}
  #     - NEURAL_API_PORT=${NEURAL_API_PORT}
